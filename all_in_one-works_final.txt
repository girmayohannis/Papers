import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
import joblib

## Assuming you have a CSV file named 'TAMIL_TRAINING_DATA.csv'
csv_file_path = r"C:\Users\girma\Desktop\NLP\share_task\tamil_n.csv"

# Read the CSV file into a DataFrame
sample = pd.read_csv(csv_file_path, encoding="latin-1")
df = sample.dropna(axis=1, how='all')
raw = df.where(pd.notnull(df), '')

# Labeling
raw.loc[raw['LABELS'] == 'stressed', 'LABELS'] = 0  # stressed=0 and non stressed=1
raw.loc[raw['LABELS'] == 'Non stressed', 'LABELS'] = 1

# Convert the 'LABELS' column to integers with handling empty strings
raw['LABELS'] = pd.to_numeric(raw['LABELS'], errors='coerce')
raw['LABELS'].fillna(0, inplace=True)  # Fill missing values with 0 (or any other suitable value)

# Assuming you have 'LABELS' and 'Text data' columns in your training data
X_train = raw['TEXT']
y_train = raw['LABELS']

# Feature extraction (TF-IDF) for training data
feature_extra = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True)
X_train_feature = feature_extra.fit_transform(X_train)

# Train a logistic regression model
classifier = LogisticRegression()
classifier.fit(X_train_feature, y_train)

# Save the trained model and feature extractor
joblib.dump(classifier, 'logistic_regression_modeltamil.joblib')
joblib.dump(feature_extra, 'tfidf_vectorizertamil.joblib')

# Load the external test data into a DataFrame
external_test_path = r"C:\Users\girma\Desktop\NLP\share_task\full_tamil_data_test.tsv"
external_test_data = pd.read_table(external_test_path, encoding="latin-1")
external_test_raw = external_test_data.dropna(axis=1, how='all')
external_test_raw = external_test_raw.where(pd.notnull(external_test_raw), '')

# Separate the features and labels for external test data
X_external_test = external_test_raw['Text data']
pid=external_test_raw['PID']

# Load the saved feature extractor and transform the external test data
loaded_feature_extra = joblib.load('tfidf_vectorizertamil.joblib')
X_external_test_feature = loaded_feature_extra.transform(X_external_test)

# Convert to dense array for your custom Logistic Regression
X_external_test_feature = X_external_test_feature.toarray()

# Load the trained logistic regression model
loaded_classifier = joblib.load('logistic_regression_modeltamil.joblib')

# Make predictions on the external test data using the loaded model
y_pred_external_test = loaded_classifier.predict(X_external_test_feature)
#send the output to the file
#send to file
# Convert predictions to DataFrame and modify 'LABELS' column
y_pred_ontest_df = pd.DataFrame({'class_label': y_pred_external_test})
y_pred_ontest_df.loc[y_pred_ontest_df['class_label'] == 0, 'class_label'] = 'stressed'
y_pred_ontest_df.loc[y_pred_ontest_df['class_label'] == 1, 'class_label'] = 'Non stressed'

# Write the predictions to a CSV file
output_csv_path = r"C:\Users\girma\Desktop\NLP\share_task\tamil_test_actual1.csv"
#output_csv_path1= r"C:\Users\girma\Desktop\NLP\share_task\final_1.csv"
y_pred_ontest_df.to_csv(output_csv_path, header=True, index=False)
#Yp_df.to_csv(output_csv_path1,header=True, index=False)


print("girma")

# ... (rest of your code for saving predictions to CSV, calculating metrics, etc.)



import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import joblib

# Assuming you have a CSV file named 'TAMIL_TRAINING_DATA.csv'
csv_file_path = r"C:\Users\girma\Desktop\NLP\share_task\tamil_n.csv"

# Read the CSV file into a DataFrame
sample = pd.read_csv(csv_file_path, encoding="latin-1")
df = sample.dropna(axis=1, how='all')
raw = df.where(pd.notnull(df), '')

# Labeling
raw.loc[raw['LABELS'] == 'stressed', 'LABELS'] = 0  # stressed=0 and non stressed=1
raw.loc[raw['LABELS'] == 'Non stressed', 'LABELS'] = 1

# Convert the 'LABELS' column to integers with handling empty strings
raw['LABELS'] = pd.to_numeric(raw['LABELS'], errors='coerce')
raw['LABELS'].fillna(0, inplace=True)  # Fill missing values with 0 (or any other suitable value)

# Assuming you have 'LABELS' and 'Text data' columns in your training data
X_train = raw['TEXT']
y_train = raw['LABELS']

# Feature extraction (TF-IDF) for training data
feature_extra = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True)
X_train_feature = feature_extra.fit_transform(X_train)

# Train a logistic regression model
classifier = LogisticRegression()
classifier.fit(X_train_feature, y_train)

# Save the trained model and feature extractor
joblib.dump(classifier, 'logistic_regression_modeltamil.joblib')
joblib.dump(feature_extra, 'tfidf_vectorizertamil.joblib')

# Load the external test data into a DataFrame
external_test_path = r"C:\Users\girma\Desktop\NLP\share_task\full_tamil_data_test.tsv"
external_test_data = pd.read_table(external_test_path, encoding="latin-1")
external_test_raw = external_test_data.dropna(axis=1, how='all')
external_test_raw = external_test_raw.where(pd.notnull(external_test_raw), '')

# Separate the features and labels for external test data
X_external_test = external_test_raw['Text data']
pid = external_test_raw['PID']

# Load the saved feature extractor and transform the external test data
loaded_feature_extra = joblib.load('tfidf_vectorizertamil.joblib')
X_external_test_feature = loaded_feature_extra.transform(X_external_test)

# Convert to dense array for your custom Logistic Regression
X_external_test_feature = X_external_test_feature.toarray()

# Load the trained logistic regression model
loaded_classifier = joblib.load('logistic_regression_modeltamil.joblib')

# Make predictions on the external test data using the loaded model
y_pred_external_test = loaded_classifier.predict(X_external_test_feature)
# Convert predictions to DataFrame and modify 'LABELS' column
y_pred_ontest_df = pd.DataFrame({'PID': pid, 'TAB': '\t', 'class_label': y_pred_external_test})

# Map numeric labels to string values
y_pred_ontest_df['class_label'] = y_pred_ontest_df['class_label'].map({0: 'stressed', 1: 'Non stressed'})

# Write the predictions to a TSV file
output_tsv_path = r"C:\Users\girma\Desktop\NLP\share_task\tamil_test_actual11.tsv"
y_pred_ontest_df.to_csv(output_tsv_path, header=True, index=False, sep='\t')
print("girma")

##########Simmilarity code
before preprcessing
wolaita 147309
dawuro 165175
gamo 139508
gofa 145129

after preprocessing(most of removing the numbers and other spacial characters)
wolaita 144085
###########################################################################################
dawuro 157976
###########################################################################################
gamo 132275
###########################################################################################
gofa 134550


# Importing necessary libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np
import re
from bs4 import BeautifulSoup

# Function to preprocess a document
def preprocess_document(text):
    soup = BeautifulSoup(text, "html.parser")
    [s.extract() for s in soup(['iframe', 'script'])]
    stripped_text = soup.get_text()
    stripped_text = re.sub(r'[\r|\n|\r\n|\s|“|”|:|(|)|-|0-9]+', '\n', stripped_text)
    return stripped_text.lower()  # Convert to lowercase

# Define three sample documents
path1 = r"E:\girma ongoing research\Omotic_data\MAATIYOOSA_wolaita.txt"
path2 = r"E:\girma ongoing research\Omotic_data\Maatoossa_dawuro.txt"
path3 = r"E:\girma ongoing research\Omotic_data\Matoosa_gamo.txt"
path4 = r"E:\girma ongoing research\Omotic_data\matoossa_gofa.txt"

# Read and preprocess documents
doc1 = preprocess_document(open(path1, encoding='latin-1').read())
doc2 = preprocess_document(open(path2, encoding='latin-1').read())
doc3 = preprocess_document(open(path3, encoding='latin-1').read())
doc4 = preprocess_document(open(path4, encoding='latin-1').read())

# Create a list of documents
documents = [doc1, doc2, doc3, doc4]

# Initialize the CountVectorizer
vectorizer = TfidfVectorizer()

# Transform the documents into a BoW matrix
tfidf_matrix = vectorizer.fit_transform(documents)

# Get the vocabulary (words) and their corresponding indices
vocabulary = vectorizer.get_feature_names_out()

# Calculate cosine similarity between documents
similarities = cosine_similarity(tfidf_matrix)

# Print the tf-idf matrix
print("tfidf-of-Words Matrix:")
print(tfidf_matrix.toarray())
print("\nVocabulary (Words):")
print(vocabulary)
print("====================================")

# Print the similarity matrix
print("Cosine Similarity Matrix:")
print(similarities)


print("\n====================================\ncomparing their distances")
# Define the tf-idf representations of the documents
bow_doc1 = tfidf_matrix[0].toarray().flatten()
bow_doc2 = tfidf_matrix[1].toarray().flatten()
bow_doc3 = tfidf_matrix[2].toarray().flatten()
bow_doc4 = tfidf_matrix[3].toarray().flatten()


# Calculate Euclidean distances
distance_doc1_doc2 = np.linalg.norm(bow_doc1 - bow_doc2)
distance_doc1_doc3 = np.linalg.norm(bow_doc1 - bow_doc3)
distance_doc1_doc4 = np.linalg.norm(bow_doc1 - bow_doc4)
distance_doc2_doc3 = np.linalg.norm(bow_doc2 - bow_doc3)
distance_doc2_doc4 = np.linalg.norm(bow_doc2 - bow_doc4)
distance_doc3_doc4 = np.linalg.norm(bow_doc3 - bow_doc4)
# Print the distances
print(f"The Distance between wolaita and dawuro: {distance_doc1_doc2:.2f}")
print(f"The Distance between wolaita and gamo: {distance_doc1_doc3:.2f}")
print(f"The Distance between wolaita and gofa: {distance_doc1_doc4:.2f}")
print(f"Then Distance between dawuro and gamo: {distance_doc2_doc3:.2f}")
print(f"Then Distance between gawuro and gofa: {distance_doc2_doc3:.2f}")
print(f"Then Distance between gamo and gofa: {distance_doc3_doc4:.2f}")


print("\n====================================\ndepict in xy coordinate")
tfidf_vectors = tfidf_matrix.toarray()
# Apply PCA to reduce dimensionality to 2 for visualization
pca = PCA(n_components=2)
pca_vectors = pca.fit_transform(tfidf_vectors)

# Plot the documents in 2D space
plt.scatter(pca_vectors[:, 0], pca_vectors[:, 1])
plt.title('Document Visualization in 2D Space')
plt.xlabel('X axis')
plt.ylabel('Y axis')

# Annotate points with document labels
for i, txt in enumerate([doc1, doc2, doc3,doc4]):
    plt.annotate(txt, (pca_vectors[i, 0], pca_vectors[i, 1]))
plt.show()
tfidf-of-Words Matrix:
[[0.05471008 0.         0.         ... 0.         0.         0.00110783]
 [0.06364182 0.00138482 0.00830893 ... 0.         0.00138482 0.        ]
 [0.         0.         0.         ... 0.00147405 0.         0.        ]
 [0.03739627 0.         0.         ... 0.         0.         0.00112663]]

Vocabulary (Words):
['aa' 'aabaan' 'aabba' ... 'zuppetokkokko' 'zuuzummeeddino'
 'zuuzummidosona']
====================================
Cosine Similarity Matrix:
[[1.         0.3128088  0.14648536 0.33377921]
 [0.3128088  1.         0.12070286 0.32069047]
 [0.14648536 0.12070286 1.         0.22125554]
 [0.33377921 0.32069047 0.22125554 1.        ]]

====================================
comparing their distances
The Distance between wolaita and dawuro: 1.17
The Distance between wolaita and gamo: 1.31
The Distance between wolaita and gofa: 1.15
Then Distance between dawuro and gamo: 1.33
Then Distance between gawuro and gofa: 1.33
Then Distance between gamo and gofa: 1.25
